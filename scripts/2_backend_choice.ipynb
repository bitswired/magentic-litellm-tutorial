{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Your Backend\n",
    "\n",
    "Magentic is compatible with many backends, including OpenAI, Anthropic, LiteLLM, ...\n",
    "\n",
    "In this tutorial, I will show you how to choose your backend. \n",
    "\n",
    "Specifically, I will demonstrate how to use LiteLLM, as I believe it's the most powerful one. It gives you access to 100+ LLMs with a unified API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out some Pydantic warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimzer/Library/Caches/pypoetry/virtualenvs/prometeeos-api-5W7JRlnu-py3.12/lib/python3.12/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/jimzer/Library/Caches/pypoetry/virtualenvs/prometeeos-api-5W7JRlnu-py3.12/lib/python3.12/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from magentic import prompt\n",
    "from magentic.chat_model.litellm_chat_model import LitellmChatModel\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Claude, it's nice to meet you! I'm Claude, an AI assistant.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@prompt(\"Say hello to {name}\", model=LitellmChatModel(\"claude-3-opus-20240229\"))\n",
    "def hello_anthropic(name: str) -> str: ...\n",
    "\n",
    "\n",
    "hello_anthropic(\"Claude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello GPT! How are you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@prompt(\"Say hello to {name}\", model=LitellmChatModel(\"gpt-3.5-turbo\"))\n",
    "def hello_openai(name: str) -> str: ...\n",
    "\n",
    "res = hello_openai(\"GPT\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@prompt(\"Say hello to {name}\")\n",
    "def hello_env(name: str) -> str: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, GPT! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"MAGENTIC_BACKEND\"] = \"litellm\"\n",
    "os.environ[\"MAGENTIC_LITELLM_MODEL\"] = \"gpt-3.5-turbo\"\n",
    "hello_env(\"GPT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Claude, it's nice to meet you! I'm an AI assistant. How are you doing today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"MAGENTIC_BACKEND\"] = \"litellm\"\n",
    "os.environ[\"MAGENTIC_LITELLM_MODEL\"] = \"claude-3-opus-20240229\"\n",
    "hello_env(\"Claude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeeos-api-5W7JRlnu-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
